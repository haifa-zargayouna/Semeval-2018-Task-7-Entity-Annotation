<?xml version="1.0" encoding="UTF-8"?>

<corpus><paper id="P06-1012" for="P06-1010"><title>

Estimating <entity>Class Priors</entity> In <entity>Domain Adaptation</entity> For <entity>Word Sense Disambiguation</entity></title><abstract>

Instances of a <entity>word</entity> drawn from different <entity>domains</entity> may have different <entity>sense priors</entity> (the proportions of the different <entity>senses</entity> of a <entity>word</entity>). This in turn affects the accuracy of <entity>word sense disambiguation (WSD) systems</entity> trained and applied on different <entity>domains</entity>. This paper presents a method to estimate the <entity>sense priors</entity> of <entity>words</entity> drawn from a new <entity>domain</entity>, and highlights the importance of using well calibrated <entity>probabilities</entity> when performing these <entity>estimations</entity>. By using well calibrated <entity>probabilities</entity>, we are able to estimate the <entity>sense priors</entity> effectively to achieve significant improvements in <entity>WSD</entity> accuracy.
</abstract></paper>

<paper id="C86-1105" for="C86-1106"><title>
An Attempt To <entity>Automatic Thesaurus Construction</entity> From An Ordinary <entity>Japanese Language Dictionary</entity></title><abstract>
How to obtain <entity>hierarchical relations</entity>(e.g. superordinate <entity>-hyponym relation</entity>, <entity>synonym relation</entity>) is one of the most important Problems for <entity>thesaurus construction</entity>. A pilot system for extracting these <entity>relations</entity> automatically from an ordinary <entity>Japanese language dictionary</entity> ( <PERSON>Shinmeikai Koku-gojiten</PERSON> , published by Sansei-do, in machine readable fori)
</abstract></paper>

<paper id="C86-1021"><title>
The <entity>Transfer Phase</entity> Of <entity>Mu Machine Translation System</entity></title><abstract>
The <entity>interlingual approach to MT</entity> has been repeatedly advocated by researchers originally interested in <entity>natural language understanding</entity> who take <entity>machine translation</entity> to be one possible application. However, not only the <entity>ambiguity</entity> but also the vagueness which every <entity>natural language</entity> inevitably has leads this approach into essential difficulties. In contrast, our project, the <entity>Mu-project</entity>, adopts the <entity>transfer approach</entity> as the basic framework of <entity>MT</entity>. This paper describes the detailed construction of the <entity>transfer phase</entity> of our <entity>system</entity> from <entity>Japanese</entity> to <entity>English</entity>, and gives some examples of problems which seem difficult to treat in the <entity>interlingual approach</entity>. The basic design principles of the <entity>transfer phase</entity> of our <entity>system</entity> have already been mentioned in (1) (2). Some of the principles which are relevant to the topic of this paper are: (a) <entity>Multiple Layer of Grammars</entity> (b) <entity>Multiple Layer Presentation</entity> (c) <entity>Lexicon  Driven Processing</entity> (d) <entity>Form-Oriented Dictionary Description</entity>. This paper also shows how these principles are realized in the current system. <entity>annotation parts</entity> of the <entity>dependent nodes</entity>, which are to be utilized in the subsequent <entity>recursion steps</entity>. For <entity>language pairs</entity> such as <entity>Japanese and English</entity> which belong to quite different <entity>language families</entity>, however, the <entity>lexical transfer</entity> is not so straightforward. It often happens that single <entity>lexical items</entity> of <entity>SL</entity> correspond to <entity>complex expressions</entity> of <entity>TL</entity> and vice versa. Furthermore, certain <entity>structural changes</entity> are also required. Because the <entity>M TP</entity> recursively transfers <entity>sub-structures</entity> governed by single <entity>nodes</entity>, certain global structural differences cannot be naturally treated. Such global changes are dealt with by the <entity>Pre-TP and Post-TP sub-phases</entity> (See section 7). W. ..... g_
</abstract></paper>

<paper id="C04-1058"><title>

Why Nitpicking Works: Evidence For Occam's Razor In <entity>Error Correctors</entity>

</title><abstract>
"Empirical experience and observations have shown us when powerful and highly tunable <entity>classifiers</entity> such as <entity>maximum entropy classifiers</entity>, <entity>boosting</entity> and <entity>SVMs</entity> are applied to <entity>language processing tasks</entity>, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various <entity>error correction mechanisms</entity> have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the <entity>test set</entity>, they are as likely to degrade accuracy as to improve it. This problem is especially severe if the base <entity>classifier</entity> has already been finely tuned. In recent work, we introduced <entity>N-fold Templated Piped Correction, or NTPC (""nitpick"")</entity>, an intriguing <entity>error corrector</entity> that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the accuracy of existing highly accurate base <entity>models</entity>. This paper investigates some of the more surprising claims made by <entity>NTPC</entity>, and presents experiments supporting an Occam's Razor argument that more complex models are damaging or unnecessary in practice. "
</abstract></paper>

<paper id="N06-1007"><title><entity>Acquisition</entity> Of <entity>Verb Entailment</entity> From <entity>Text</entity></title><abstract>
The study addresses the problem of <entity>automatic acquisition</entity> of <entity>entailment relations</entity> between <entity>verbs</entity>. While this task has much in common with <entity>paraphrases acquisition</entity> which aims to discover <entity>semantic equivalence</entity> between <entity>verbs</entity>, the main  challenge of <entity>entailment acquisition</entity> is to capture <entity>asymmetric, or directional, relations</entity>. Motivated by the intuition that it often underlies the <entity>local structure</entity> of <entity>coherent text</entity>, we develop a method that discovers <entity>verb entailment</entity> using evidence about <entity>discourse relations</entity> between <entity>clauses</entity> available in a <entity>parsed corpus</entity>. In comparison with earlier work, the proposed method covers a much wider range of <entity>verb entailment types</entity> and learns the <entity>mapping</entity> between <entity>verbs</entity> with highly varied <entity>argument structures</entity>.
</abstract></paper>

<paper id="A00-2023"><title><entity>Forest-Based Statistical Sentence Generation</entity></title><abstract>
This paper presents a new approach to <entity>statistical sentence generation</entity> in which alternative <entity>phrases</entity> are represented as <entity>packed sets of trees, or forests</entity>, and then ranked statistically to choose the best one. This <entity>representation</entity> offers advantages in compactness and in the ability to represent <entity>syntactic information</entity>. It also facilitates more efficient <entity>statistical ranking</entity> than a previous approach to <entity>statistical generation</entity>. An efficient <entity>ranking algorithm</entity> is described, together with experimental results showing significant improvements over <entity>simple enumeration</entity> or a <entity>lattice-based approach</entity>.
</abstract></paper>

<paper id="X98-1022"><title>
An <entity>NTU-Approach</entity> To <entity>Automatic Sentence Extraction</entity> For <entity>Summary Generation</entity></title>
<abstract><entity>Automatic summarization</entity> and <entity>information extraction</entity> are two important Internet services. <entity>MUC</entity> and <entity>SUMMAC</entity> play their appropriate roles in the next generation Internet. This paper focuses on the <entity>automatic summarization</entity> and proposes two different models to extract <entity>sentences</entity> for <entity>summary generation</entity> under two tasks initiated by <entity>SUMMAC-1</entity>. For <entity>categorization task</entity>, <entity>positive feature vectors</entity> and <entity>negative feature vectors</entity> are used cooperatively to construct generic, indicative <entity>summaries</entity>. For adhoc task, a <entity>text model</entity> based on relationship between <entity>nouns</entity> and <entity>verbs</entity> is used to filter out irrelevant <entity>discourse segment</entity>, to rank relevant <entity>sentences</entity>, and to generate the <entity>user-directed summaries</entity>. The result shows that the <entity>NormF</entity> of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0. 447. The <entity>NormF</entity> of the best summary and that of the fixed summary for <entity>categorization task</entity> are 0.4090 and 0.4023. Our system outperforms the average system in <entity>categorization task</entity> but does a common job in adhoc task.
</abstract></paper>

<paper id="P98-2213" for="P98-2215"><title>

A <entity>Method</entity> for Relating Multiple Newspaper Articles by Using <entity>Graphs</entity>, and Its Application to <entity>Webcasting</entity>

</title><abstract>

This paper describes methods for relating (threading) multiple newspaper articles, and for visualizing various characteristics of them by using a <entity>directed graph</entity>. A set of articles is represented by a set of <entity>word vectors</entity>, and the <entity>similarity</entity> between the <entity>vectors</entity> is then calculated. The graph is constructed from the <entity>similarity matrix</entity>. By applying some <entity>constraints</entity> on the chronological ordering of articles, an efficient <entity>threading algorithm</entity> that runs in <entity>0(n) time</entity> (where n is the number of articles) is obtained. The constructed <entity>graph</entity> is visualized with <entity>words</entity> that represent the <entity>topics</entity> of the <entity>threads</entity>, and <entity>words</entity> that represent new <entity>information</entity> in each article. The <entity>threading technique</entity> is suitable for <entity>Webcasting (push) applications</entity>. A <entity>threading server</entity> determines relationships among articles from various news sources, and creates files containing their <entity>threading information</entity>. This information is represented in <entity>eXtended Markup Language (XML)</entity>, and can be visualized on most Web browsers. The <entity>XML-based representation</entity> and a current prototype are described in this paper.

</abstract></paper>

<paper id="P98-1113"><title>

A Flexible <entity>Example-Based Parser</entity> Based on the <entity>SST C</entity>

</title><abstract>

In this paper we sketch an approach for <entity>Natural Language parsing</entity>. Our approach is an <entity>example-based approach</entity>, which relies mainly on examples that already parsed to their <entity>representation structure</entity>, and on the knowledge that we can get from these examples the required information to parse a new <entity>input sentence</entity>. In our approach, examples are annotated with the <entity>Structured String Tree Correspondence (SSTC) annotation schema</entity> where each <entity>SSTC</entity> describes a <entity>sentence</entity>, a <entity>representation tree</entity> as well as the correspondence between <entity>substrings</entity> in the <entity>sentence</entity> and <entity>subtrees</entity> in the <entity>representation tree</entity>. In the process of <entity>parsing</entity>, we first try to build <entity>subtrees</entity> for <entity>phrases</entity> in the <entity>input sentence</entity> which have been successfully found in the <entity>example-base</entity> - a <entity>bottom up approach</entity>. These <entity>subtrees</entity> will then be combined together to form a <entity>single rooted representation tree</entity> based on an example with similar <entity>representation structure</entity> - a <entity>top down approach</entity>.Keywords:

</abstract></paper>
</corpus>
