<?xml version="1.0" encoding="UTF-8"?>
<corpus>
<paper id="P02-1059"><title>
Supervised Ranking In Open-<entity>Domain</entity> <entity>Text Summarization</entity></title><abstract>
The <entity>paper</entity> proposes and empirically motivates an <entity>integration</entity> of supervised <entity>learning</entity> with unsupervised <entity>learning</entity> to <entity>deal</entity> with human <entity>biases</entity> in <entity>summarization</entity>. In particular, we explore the use of probabilistic <entity>decision tree</entity> within the <entity>clustering</entity> <entity>framework</entity> to account for the <entity>variation</entity> as well as <entity>regularity</entity> in human created <entity>summaries</entity>. The <entity>corpus</entity> of human created <entity>extracts</entity> is created from a <entity>newspaper</entity> <entity>corpus</entity> and used as a <entity>test set</entity>. We build probabilistic <entity>decision trees</entity> of different flavors and integrate each of them with the <entity>clustering</entity> <entity>framework</entity>. <entity>Experiments</entity> with the <entity>corpus</entity> demonstrate that the <entity>mixture</entity> of the two <entity>paradigms</entity> generally gives a significant boost in <entity>performance</entity> compared to <entity>cases</entity> where either ofthe two is considered alone.
</abstract></paper>
<paper id="P02-1002"><title>
Sequential Conditional <entity>Generalized</entity> Iterative <entity>Scaling</entity></title><abstract>
We describe a speedup for <entity>training</entity> conditional <entity>maximum entropy models</entity>. The <entity>algorithm</entity> is a <entity>simple</entity> <entity>variation</entity> on <entity>Generalized</entity> Iterative <entity>Scaling</entity>, but converges roughly an <entity>order</entity> of <entity>magnitude</entity> faster, depending on the <entity>number</entity> of <entity>constraints</entity>, and the way <entity>speed</entity> is measured. Rather than attempting to <entity>train</entity> all <entity>model parameters</entity> simultaneously, the <entity>algorithm</entity> <entity>trains</entity> them sequentially. The <entity>algorithm</entity> is easy to <entity>implement</entity>, typically uses only slightly more <entity>memory</entity>, and will lead to <entity>improvements</entity> for most <entity>maximum entropy</entity> <entity>problems</entity>.
</abstract></paper>
<paper id="C00-2123"><title><entity>Word</entity> Re-Ordering And DP-Based <entity>Search</entity> In <entity>Statistical Machine Translation</entity></title><abstract>
In this <entity>paper</entity>, we describe a <entity>search</entity> <entity>procedure</entity> for <entity>statistical machine translation</entity> (MT) <entity>based</entity> on <entity>dynamic programming</entity> (DP). Starting from a DP-based <entity>solution</entity> to the traveling salesman <entity>problem</entity>, we present a novel <entity>technique</entity> to restrict the possible <entity>word</entity> reordering between <entity>source</entity> and <entity>target language</entity> in <entity>order</entity> to achieve an efficient <entity>search algorithm</entity>. A <entity>search</entity> <entity>restriction</entity> especially useful for the <entity>translation</entity> <entity>direction</entity> from German to <entity>English</entity> is presented. The <entity>experimental</entity> <entity>tests</entity> are carried out on the Verbmobil <entity>task</entity> (German-<entity>English</entity>, 8000-<entity>word</entity> <entity>vocabulary</entity>), which is a <entity>limited-domain</entity> <entity>spoken-language</entity> <entity>task</entity>.
</abstract></paper>

<paper id="C96-1055"><title><entity>Role</entity> Of <entity>Word Sense Disambiguation</entity> In <entity>Lexical</entity> <entity>Acquisition</entity>: Predicting <entity>Semantics</entity> From <entity>Syntactic</entity> Cues
</title><abstract>
This <entity>paper</entity> addresses the <entity>issue</entity> of <entity>word-sense</entity> <entity>ambiguity</entity> in <entity>extraction</entity> from <entity>machine-readable</entity> <entity>resources</entity> for the <entity>construction</entity> of <entity>large-scale</entity> <entity>knowledge sources</entity>. We describe two <entity>experiments</entity>: one which ignored <entity>word-sense</entity> <entity>distinctions</entity>, <entity>resulting</entity> in 6.3% <entity>accuracy</entity> for <entity>semantic</entity> <entity>classification</entity> of <entity>verbs</entity> <entity>based</entity> on ( <BIBLIO>Levin, 1993</BIBLIO> ); and one which exploited <entity>word-sense</entity> <entity>distinctions</entity>, <entity>resulting</entity> in 97.9% <entity>accuracy</entity>. These <entity>experiments</entity> were dual <entity>purpose</entity>: (1) to validate the central <entity>thesis</entity> of the work of ( <BIBLIO>Levin, 1993</BIBLIO> ), i.e., that <entity>verb</entity> <entity>semantics</entity> and <entity>syntactic</entity> <entity>behavior</entity> are predictably related; (2) to demonstrate that a 15-fold <entity>improvement</entity> can be achieved in deriving <entity>semantic information</entity> from <entity>syntactic</entity> <entity>cues</entity> if we first divide the <entity>syntactic</entity> <entity>cues</entity> into distinct groupings that correlate with different <entity>word</entity> senses. Finally, we show that we can <entity>provide</entity> effective <entity>acquisition</entity> <entity>techniques</entity> for novel <entity>word</entity> senses using a <entity>combination</entity> of online <entity>sources</entity>.
</abstract></paper>

<paper id="M91-1029"><title>
PRC Inc: <entity>Description</entity> Of The PAKTUS <entity>System</entity> Used For MUC-3
</title><abstract>
The PRC Adaptive <entity>Knowledge-based</entity> <entity>Text</entity> <entity>Understanding System</entity> (PAKTUS) has been under <entity>development</entity> as an Independent <entity>Research</entity> and <entity>Development</entity> <entity>project</entity> at PRC since 1984. The <entity>objective</entity> is a generic <entity>system</entity> of <entity>tools</entity>, <entity>including</entity> a <entity>core</entity> <entity>English</entity> <entity>lexicon</entity>, grammar, and <entity>concept</entity> <entity>representations</entity>, for <entity>building</entity> <entity>natural language processing</entity> (NLP) <entity>systems</entity> for <entity>text</entity> <entity>understanding</entity>. <entity>Systems</entity> built with PAKTUS are intended to <entity>generate</entity> <entity>input</entity> to <entity>knowledge based</entity> <entity>systems</entity> or<entity>data</entity> <entity>base</entity> <entity>systems</entity>. <entity>Input</entity> to the <entity>NLP system</entity> is typically derived from an existing electronic <entity>message</entity> <entity>stream</entity>, such as a <entity>news</entity> wire. PAKTUS <entity>supports</entity> the <entity>adaptation</entity> of the generic <entity>core</entity> to a <entity>variety</entity> of <entity>domains</entity>: JINTACCS <entity>messages</entity>, RAINFORM <entity>messages</entity>, <entity>news</entity> <entity>reports</entity> about a specific <entity>type</entity> of <entity>event</entity>, such as financial <entity>transfers</entity> or terrorist acts, etc., by acquiring sublanguage and <entity>domain-specific</entity> grammar, <entity>words</entity>, conceptual <entity>mappings</entity>, and <entity>discourse</entity> <entity>patterns</entity>. The <entity>long-term</entity> <entity>goal</entity> is a <entity>system</entity> that can <entity>support</entity> the <entity>processing</entity> of relatively long <entity>discourses</entity> in <entity>domains</entity> that are fairly broad with a high <entity>rate</entity> of <entity>success</entity>.
</abstract></paper>

<paper id="H93-1113"><title><entity>Natural Language</entity> <entity>Research</entity></title><abstract>
The <entity>main</entity> <entity>objective</entity> is <entity>basic</entity> <entity>research</entity> and <entity>system</entity> <entity>development</entity> leading to (1) <entity>characterization</entity> of <entity>information</entity> carried by (a) <entity>syntax</entity>, <entity>semantics</entity>, and <entity>discourse structure</entity>, (b) their <entity>relation</entity> to <entity>information</entity> carried by <entity>intonation</entity>, and (c) <entity>development</entity> of <entity>methods</entity> for using this <entity>information</entity> for <entity>generation</entity> and <entity>understanding</entity>; (2) <entity>development</entity> of <entity>architectures</entity> for <entity>integration</entity> of <entity>utterance</entity> planning with <entity>lexical</entity>, <entity>syntactic</entity> and intonational <entity>choice</entity>; (3) <entity>development</entity> of incremental <entity>strategies</entity> for using <entity>syntactic</entity>, <entity>semantic</entity>, and pragmatic <entity>knowledge</entity> in <entity>understanding</entity> and <entity>generating</entity> <entity>language</entity>.
</abstract></paper>

<paper id="P98-1088"><title>
Memoisation for Glue <entity>Language</entity> <entity>Deduction</entity> and Categorial <entity>Parsing</entity></title><abstract>
"The multiplicative <entity>fragment</entity> of <entity>linear logic</entity> has found a <entity>number</entity> of <entity>applications</entity> in <entity>computational linguistics</entity>: in the ""glue <entity>language</entity>"" <entity>approach</entity> to LFG <entity>semantics</entity>, and in the <entity>formulation</entity> and <entity>parsing</entity> of various <entity>categorial grammars</entity>. These <entity>applications</entity> <entity>call</entity> for efficient <entity>deduction</entity> <entity>methods</entity>. Although a <entity>number</entity> of <entity>deduction</entity> <entity>methods</entity> for multiplicative <entity>linear logic</entity> are known, none of them are tabular <entity>methods</entity>, which bring a substantial <entity>efficiency</entity> <entity>gain</entity> by avoiding redundant <entity>computation</entity> (cf. chart <entity>methods</entity> in CFG <entity>parsing</entity>): this <entity>paper</entity> presents such a <entity>method</entity>, and discusses its use in <entity>relation</entity> to the above <entity>applications</entity>."
</abstract></paper>

<paper id="E06-1045"><title><entity>Data-</entity>Driven <entity>Generation</entity> Of Emphatic Facial Displays
</title><abstract>
We describe an <entity>implementation</entity> of <entity>data-driven</entity> <entity>selection</entity> of emphatic facial <entity>displays</entity> for an embodied conversational <entity>agent</entity> in a <entity>dialogue system</entity>. A <entity>corpus</entity> of <entity>sentences</entity> in the <entity>domain</entity> of the <entity>target</entity> <entity>dialogue system</entity> was <entity>recorded</entity>, and the facial <entity>displays</entity> used by the speaker were annotated. The<entity>data</entity> from those <entity>recordings</entity> was used in a range of <entity>models</entity> for <entity>generating</entity> facial <entity>displays</entity>, each <entity>model</entity> making use of a different <entity>amount</entity> of <entity>context</entity> or choosing <entity>displays</entity> differently within a <entity>context</entity>. The <entity>models</entity> were <entity>evaluated</entity> in two ways: by <entity>cross-validation</entity> against the <entity>corpus</entity>, and by asking <entity>users</entity> to <entity>rate</entity> the <entity>output</entity>. The <entity>predictions</entity> of the <entity>cross-validation</entity> <entity>study</entity> differed from the actual <entity>user</entity> <entity>ratings</entity>. While the <entity>cross-validation</entity> gave the highest scores to <entity>models</entity> making a <entity>majority</entity> <entity>choice</entity> within a <entity>context</entity>, the <entity>user</entity> <entity>study</entity> showed a significant <entity>preference</entity> for <entity>models</entity> that produced more <entity>variation</entity>. This <entity>preference</entity> was especially strong among the female subjects.
</abstract></paper>

</corpus>