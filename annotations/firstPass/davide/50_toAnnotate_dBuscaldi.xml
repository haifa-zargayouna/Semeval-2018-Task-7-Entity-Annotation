<?xml version="1.0" encoding="UTF-8"?>
<corpus>
<paper id="P04-1030"><title>
Head-Driven <entity>Parsing</entity> For <entity>Word</entity> Lattices
</title><abstract>
We present the first <entity>application</entity> of the head-driven <entity>statistical</entity> <entity>parsing</entity> <entity>model</entity> of <PERSON>Collins</PERSON> (1999) as a simultaneous <entity>language model</entity> and <entity>parser</entity> for <entity>large-vocabulary</entity> <entity>speech recognition</entity>. The <entity>model</entity> is <entity>adapted</entity> to an online left to right <entity>chart-parser</entity> for <entity>word</entity> <entity>lattices</entity>, integrating acoustic, <entity>n-gram</entity>, and <entity>parser</entity> <entity>probabilities</entity>. The <entity>parser</entity> uses <entity>structural</entity> and <entity>lexical</entity> <entity>dependencies</entity> not considered by <entity>n-gram models</entity>, conditioning <entity>recognition</entity> on more linguistically-grounded <entity>relationships</entity>. <entity>Experiments</entity> on the <entity>Wall Street Journal</entity> treebank and <entity>lattice</entity> <entity>corpora</entity> show <entity>word error rates</entity> competitive with the <entity>standard</entity> n
</abstract></paper>

<paper id="P02-1023"><title>
Improving <entity>Language Model</entity> <entity>Size</entity> <entity>Reduction</entity> Using Better Pruning Criteria
</title><abstract>
Reducing <entity>language model</entity> (LM) <entity>size</entity> is a critical <entity>issue</entity> when <entity>applying</entity> a LM to realistic <entity>applications</entity> which have <entity>memory</entity> <entity>constraints</entity>. In this <entity>paper</entity>, three measures are <entity>studied</entity> for the <entity>purpose</entity> of LM pruning. They are <entity>probability</entity>, <entity>rank</entity>, and <entity>entropy</entity>. We <entity>evaluated</entity> the <entity>performance</entity> of the three pruning <entity>criteria</entity> in a real <entity>application</entity> of <entity>Chinese</entity> <entity>text</entity> <entity>input</entity> in <entity>terms</entity> of character <entity>error rate</entity> (CER). We first present an empirical <entity>comparison</entity>, showing that <entity>rank</entity> <entity>performs</entity> the best in most <entity>cases</entity>. We also show that the <entity>high-performance</entity> of <entity>rank</entity> lies in its strong <entity>correlation</entity> with <entity>error rate</entity>. We then present a novel <entity>method</entity> of combining two <entity>criteria</entity> in <entity>model</entity> pruning. <entity>Experimental</entity> <entity>results</entity> show that the combined <entity>criterion</entity> consistently leads to smaller <entity>models</entity> than the <entity>models</entity> pruned using either of the <entity>criteria</entity> separately, at the same CER.
</abstract></paper>

<paper id="C00-1054"><title>
Finite-State Multimodal <entity>Parsing</entity> And <entity>Understanding</entity></title><abstract>
Multimodal <entity>interfaces</entity> <entity>require</entity> effective <entity>parsing</entity> and understanding of <entity>utterances</entity> whose <entity>content</entity> is <entity>distributed</entity> across multiple <entity>input</entity> <entity>modes</entity>. <BIBLIO>Johnston 1998</BIBLIO>  presents an <entity>approach</entity> in which <entity>strategies</entity> for multimodal <entity>integration</entity> are stated declaratively using a <entity>unification-based</entity> grammar that is used by a multidimensional chart <entity>parser</entity> to compose <entity>inputs</entity>. This <entity>approach</entity> is highly expressive and <entity>supports</entity> a broad <entity>class</entity> of <entity>interfaces</entity>, but offers only limited potential for mutual compensation among the <entity>input</entity> <entity>modes</entity>, is subject to significant <entity>concerns</entity> in <entity>terms</entity> of <entity>computational</entity> <entity>complexity</entity>, and complicates <entity>selection</entity> among <entity>alternative</entity> multimodal <entity>interpretations</entity> of the <entity>input</entity>. In this <entity>paper</entity>, we present an <entity>alternative</entity> <entity>approach</entity> in which multimodal <entity>parsing</entity> and understanding are achieved using a weighted finite-state <entity>device</entity> which takes <entity>speech</entity> and <entity>gesture</entity> <entity>streams</entity> as <entity>inputs</entity> and <entity>outputs</entity> their joint <entity>interpretation</entity>. This <entity>approach</entity> is significantly more efficient, enables tight-coupling of multimodal <entity>understanding</entity> with <entity>speech recognition</entity>, and <entity>provides</entity> a general probabilistic <entity>framework</entity> for multimodal <entity>ambiguity</entity> <entity>resolution</entity>.
</abstract></paper>

<paper id="C04-1103"><title>
Direct Orthographical <entity>Mapping</entity> For <entity>Machine</entity> <entity>Transliteration</entity></title><abstract><entity>Machine</entity> <entity>transliteration</entity>/<entity>back-transliteration</entity> plays an important <entity>role</entity> in many multilingual <entity>speech</entity> and <entity>language</entity> <entity>applications</entity>. In this <entity>paper</entity>, a novel <entity>framework</entity> for <entity>machine</entity> <entity>transliteration</entity>/backtransliteration that allows us to carry out direct orthographical <entity>mapping</entity> (<entity>DOM</entity>) between two different <entity>languages</entity> is presented. Under this <entity>framework</entity>, a joint <entity>source-channel</entity> <entity>transliteration</entity> <entity>model</entity>, also <entity>called</entity> 
-<entity>gram</entity> <entity>transliteration</entity> <entity>model</entity> (<entity>n-gram</entity> TM), is further proposed to <entity>model</entity> the <entity>transliteration</entity> <entity>process</entity>. We <entity>evaluate</entity> the proposed <entity>methods</entity> through several <entity>transliteration</entity>/backtransliteration <entity>experiments</entity> for <entity>English</entity>/<entity>Chinese</entity> and <entity>English</entity>/<entity>Japanese</entity> <entity>language pairs</entity>. Our <entity>study</entity> reveals that the proposed <entity>method</entity> not only reduces an extensive <entity>system</entity> <entity>development</entity> <entity>effort</entity> but also <entity>improves</entity> the <entity>transliteration</entity> <entity>accuracy</entity> significantly.
</abstract></paper>

<paper id="N06-1037"><title>
Exploring <entity>Syntactic</entity> <entity>Features</entity> For <entity>Relation Extraction</entity> Using A <entity>Convolution</entity> <entity>Tree Kernel</entity></title><abstract>
This <entity>paper</entity> proposes to use a <entity>convolution</entity> kernel over <entity>parse trees</entity> to <entity>model</entity> <entity>syntactic structure</entity> <entity>information</entity> for <entity>relation extraction</entity>. Our <entity>study</entity> reveals that the <entity>syntactic structure</entity> <entity>features</entity> embedded in a <entity>parse tree</entity> are very effective for <entity>relation extraction</entity> and these <entity>features</entity> can be well captured by the <entity>convolution</entity> <entity>tree kernel</entity>. <entity>Evaluation</entity> on the <entity>ACE</entity> 2003 <entity>corpus</entity> shows that the <entity>convolution</entity> kernel over <entity>parse trees</entity> can achieve comparable <entity>performance</entity> with the previous best-reported <entity>feature-based methods</entity> on the 24 <entity>ACE</entity> <entity>relation</entity> subtypes. It also shows that our <entity>method</entity> significantly outperforms the previous two <entity>dependency tree</entity> kernels on the 5 <entity>ACE</entity> <entity>relation</entity> major <entity>types</entity>.
</abstract></paper>

<paper id="H90-1011"><title><entity>Performing</entity> Integrated <entity>Syntactic</entity> And <entity>Semantic</entity> <entity>Parsing</entity> Using <entity>Classification</entity></title><abstract>
This <entity>paper</entity> describes a particular <entity>approach</entity> to <entity>parsing</entity> that utilizes recent <entity>advances</entity> in <entity>unification-based</entity> <entity>parsing</entity> and in <entity>classification-based</entity> <entity>knowledge representation</entity>. As <entity>unification-based</entity> grammatical <entity>frameworks</entity> are extended to handle richer <entity>descriptions</entity> of <entity>linguistic information</entity>, they begin to share many of the <entity>properties</entity> that have been <entity>developed</entity> in KL-ONE-like <entity>knowledge representation</entity> <entity>systems</entity>. This commonality suggests that some of the <entity>classification-based</entity> <entity>representation</entity> <entity>techniques</entity> can be <entity>applied</entity> to <entity>unification-based</entity> <entity>linguistic descriptions</entity>. This merging <entity>supports</entity> the <entity>integration</entity> of <entity>semantic</entity> and <entity>syntactic information</entity> into the same <entity>system</entity>, simultaneously subject to the same <entity>types</entity> of <entity>processes</entity>, in an efficient <entity>manner</entity>. The <entity>result</entity> is expected to be more efficient <entity>parsing</entity> <entity>due</entity> to the <entity>increased</entity> <entity>organization</entity> of <entity>knowledge</entity>. The use of a KL-ONE style <entity>representation</entity> for <entity>parsing</entity> and <entity>semantic interpretation</entity> was first explored in the PSI-KLONE <entity>system</entity> [2], in which <entity>parsing</entity> is characterized as an <entity>inference</entity> <entity>process</entity> <entity>called</entity> incremental <entity>description</entity> <entity>refinement</entity>.
</abstract></paper>

<paper id="J97-1004"><title><entity>Developing</entity> And Empirically <entity>Evaluating</entity> <entity>Robust</entity> <entity>Explanation</entity> Generators: The <entity>KNIGHT</entity> <entity>Experiments</entity></title><abstract>
"To explain <entity>complex</entity> <entity>phenomena</entity>, an <entity>explanation</entity> <entity>system</entity> must be able to select <entity>information</entity> from a formal <entity>representation</entity> of <entity>domain knowledge</entity>, organize the selected <entity>information</entity> into multisenten-tial <entity>discourse</entity> plans, and realize the <entity>discourse</entity> plans in <entity>text</entity>. Although recent years have witnessed significant <entity>progress</entity> in the <entity>development</entity> of sophisticated <entity>computational</entity> <entity>mechanisms</entity> for <entity>explanation</entity>, empirical <entity>results</entity> have been limited. This <entity>paper</entity> <entity>reports</entity> on a seven-year <entity>effort</entity> to empirically <entity>study</entity> <entity>explanation</entity> <entity>generation</entity> from semantically rich, <entity>large-scale</entity> <entity>knowledge bases</entity>. In particular, it describes a <entity>robust</entity> <entity>explanation</entity> <entity>system</entity> that <entity>constructs</entity> multisentential and <entity>multi-paragraph</entity> <entity>explanations</entity> from the a <entity>large-scale</entity> <entity>knowledge base</entity> in the <entity>domain</entity> of botanical anatomy, physiology, and <entity>development</entity>. We introduce the <entity>evaluation</entity> <entity>methodology</entity> and describe how <entity>performance</entity> was assessed with this <entity>methodology</entity> in the most extensive empirical <entity>evaluation</entity> conducted on an <entity>explanation</entity> <entity>system</entity>. In this <entity>evaluation</entity>, scored within ""half a grade"" of <entity>domain</entity> <entity>experts</entity>, and its <entity>performance</entity> exceeded that of one of the <entity>domain</entity> <entity>experts</entity>."
</abstract></paper>

<paper id="C02-1071"><title>
Integrating Shallow Linguistic <entity>Processing</entity> Into A <entity>Unification-</entity><entity>Based</entity> Spanish Grammar
</title><abstract>
This <entity>paper</entity> describes to what <entity>extent</entity> deep <entity>processing</entity> may <entity>benefit</entity> from shallow <entity>processing</entity> <entity>techniques</entity> and it presents a <entity>NLP system</entity> which integrates a linguistic PoS tagger and chunker as a preprocessing <entity>module</entity> of a <entity>broad-coverage</entity> <entity>unification-based</entity> grammar of Spanish. <entity>Experiments</entity> show that the <entity>efficiency</entity> of the overall <entity>analysis</entity> <entity>improves</entity> significantly and that our <entity>system</entity> also <entity>provides</entity> <entity>robustness</entity> to the linguistic <entity>processing</entity>, while maintaining both the <entity>accuracy</entity> and the <entity>precision</entity> of the grammar.
</abstract></paper>

</corpus>