<?xml version="1.0" encoding="UTF-8"?>
<corpus>
<corpus><paper id="P06-1012" for="P06-1010">
<title>
Estimating <entity>Class Priors</entity> In <entity>Domain Adaptation</entity> For <entity>Word Sense Disambiguation</entity></title>
<abstract>
Instances of a <entity>word</entity> drawn from different <entity>domains</entity> may have different <entity>sense priors</entity> (the proportions of the different senses of a <entity>word</entity>). This in turn affects the <entity>accuracy</entity> of <entity>word sense disambiguation (WSD) systems</entity> trained and applied on different <entity>domains</entity>. This paper presents a method to estimate the <entity>sense priors</entity> of <entity>words</entity> drawn from a new <entity>domain</entity>, and highlights the importance of using <entity>well calibrated probabilities</entity> when performing these <entity>estimations</entity>. By using <entity>well calibrated probabilities</entity>, we are able to estimate the <entity>sense priors</entity> effectively to achieve significant improvements in <entity>WSD accuracy</entity>.
</abstract></paper>

<paper id="C86-1105" for="C86-1106">
<title>
An Attempt To <entity>Automatic Thesaurus Construction</entity> From An Ordinary <entity>Japanese Language Dictionary</entity></title>
<abstract>
How to obtain <entity>hierarchical relations</entity>(e.g. <entity>superordinate -hyponym relation</entity>, <entity>synonym relation</entity>) is one of the most important Problems for <entity>thesaurus construction</entity>.   A <entity>pilot system</entity> for extracting these <entity>relations</entity> automatically from an ordinary <entity>Japanese language dictionary</entity> ( <PERSON>Shinmeikai Koku-gojiten</PERSON> , published by Sansei-do, in machine readable fori)
</abstract></paper>

<paper id="C86-1021">
<title>
The <entity>Transfer Phase</entity> Of <entity>Mu Machine Translation System</entity></title>
<abstract>
The <entity>interlingual approach to MT</entity> has been repeatedly advocated by researchers originally interested in <entity>natural language understanding</entity> who take <entity>machine translation</entity> to be one possible application. However, not only the <entity>ambiguity</entity> but also the <entity>vagueness</entity> which every <entity>natural language</entity> inevitably has leads this approach into essential difficulties. In contrast, our project, the <entity>Mu-project</entity>, adopts the <entity>transfer approach</entity> as the basic framework of <entity>MT</entity>. This paper describes the detailed construction of the <entity>transfer phase</entity> of our system from <entity>Japanese</entity> to <entity>English</entity>, and gives some examples of problems which seem difficult to treat  in the  <entity>interlingual approach</entity>. The basic design principles of the <entity>transfer phase</entity> of our system have already been mentioned in (1) (2). Some of the principles which are relevant to the topic of  this paper are: (a) <entity>Multiple Layer of Grammars</entity> (b) <entity>Multiple Layer Presentation</entity> (c) <entity>Lexicon Driven Processing</entity> (d) <entity>Form-Oriented Dictionary Description</entity>. This paper also shows how these principles are realized in the current system.
</abstract></paper>

<paper id="C04-1058">
<title>
Why Nitpicking Works: Evidence For <entity>Occam's Razor</entity> In <entity>Error Correctors</entity>
</title>
<abstract>
"Empirical experience and observations have shown us when powerful and highly tunable <entity>classifiers</entity> such as <entity>maximum entropy classifiers</entity>, <entity>boosting</entity> and <entity>SVMs</entity> are applied to <entity>language processing tasks</entity>, it is possible to achieve high <entity>accuracies</entity>, but eventually their <entity>performances</entity> all tend to plateau out at around the same point. To further improve <entity>performance</entity>, various <entity>error correction mechanisms</entity> have been developed, but in practice, most of them cannot be relied on to predictably improve <entity>performance</entity> on <entity>unseen data</entity>; indeed, depending upon the <entity>test set</entity>, they are as likely to degrade <entity>accuracy</entity> as to improve it. This problem is especially severe if the <entity>base classifier</entity> has already been finely tuned. In recent work, we introduced <entity>N-fold Templated Piped Correction</entity>, or <entity>NTPC (""nitpick"")</entity>, an intriguing <entity>error corrector</entity> that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the <entity>accuracy</entity> of existing highly accurate <entity>base models</entity>. This paper investigates some of the more surprising claims made by <entity>NTPC</entity>, and presents experiments supporting an <entity>Occam's Razor argument</entity> that more complex <entity>models</entity> are damaging or unnecessary in practice. "
</abstract></paper>

<paper id="N06-1007"><title><entity>Acquisition</entity> Of <entity>Verb Entailment</entity> From <entity>Text</entity></title><abstract>
The study addresses the problem of <entity>automatic acquisition</entity> of <entity>entailment relations</entity> between <entity>verbs</entity>. While this task has much in common with <entity>paraphrases acquisition</entity> which aims to discover <entity>semantic equivalence</entity> between <entity>verbs</entity>, the main challenge of <entity>entailment acquisition</entity> is to capture <entity>asymmetric, or directional, relations</entity>. Motivated by the intuition that it often underlies the <entity>local structure</entity> of <entity>coherent text</entity>, we develop a method that discovers <entity>verb entailment</entity> using evidence about <entity>discourse relations</entity> between <entity>clauses</entity> available in a <entity>parsed corpus</entity>. In comparison with earlier work, the proposed method covers a much wider range of <entity>verb entailment types</entity> and learns the <entity>mapping</entity> between <entity>verbs</entity> with highly varied <entity>argument structures</entity>.
</abstract></paper>

<paper id="A00-2023"><title><entity>Forest-Based Statistical Sentence Generation</entity></title><abstract>
This paper presents a new approach to <entity>statistical sentence generation</entity> in which alternative <entity>phrases</entity> are represented as packed sets of <entity>trees</entity>, or <entity>forests</entity>, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent <entity>syntactic information</entity>. It also facilitates more efficient <entity>statistical ranking</entity> than a previous approach to <entity>statistical generation</entity>. An efficient <entity>ranking algorithm</entity> is described, together with experimental results showing significant improvements over simple enumeration or a <entity>lattice-based approach</entity>.
</abstract></paper>

<paper id="X98-1022">
<title>An NTU-Approach To <entity>Automatic Sentence Extraction</entity> For <entity>Summary Generation</entity></title>
<abstract><entity>Automatic summarization</entity> and <entity>information extraction</entity> are two important <entity>Internet services</entity>. <entity>MUC</entity> and <entity>SUMMAC</entity> play their appropriate roles in the next generation Internet. This paper focuses on the <entity>automatic summarization</entity> and proposes two different models to extract <entity>sentences</entity> for <entity>summary generation</entity> under two tasks initiated by <entity>SUMMAC-1</entity>. For <entity>categorization task</entity>, <entity>positive feature vectors</entity> and <entity>negative feature vectors</entity> are used cooperatively to construct generic, indicative <entity>summaries</entity>. For adhoc task, a <entity>text model</entity> based on relationship between <entity>nouns</entity> and <entity>verbs</entity> is used to filter out irrelevant <entity>discourse segment</entity>, to rank relevant <entity>sentences</entity>, and to generate the <entity>user-directed summaries</entity>. The result shows that the <entity>NormF</entity> of the best <entity>summary</entity> and that of the fixed <entity>summary</entity> for adhoc tasks are 0.456 and 0. 447. The <entity>NormF</entity> of the best <entity>summary</entity> and that of the fixed <entity>summary</entity> for <entity>categorization task</entity> are 0.4090 and 0.4023. Our system outperforms the average <entity>system</entity> in <entity>categorization task</entity> but does a common job in adhoc task.
</abstract></paper>

<paper id="P98-2213" for="P98-2215"><title>
A Method for Relating Multiple Newspaper Articles by Using <entity>Graphs</entity>, and Its Application to Webcasting
</title><abstract>
This paper describes methods for relating (threading) multiple newspaper articles, and for visualizing various characteristics of them by using a <entity>directed graph</entity>. A set of articles is represented by a set of <entity>word vectors</entity>, and the <entity>similarity</entity> between the <entity>vectors</entity> is then calculated. The <entity>graph</entity> is constructed from the <entity>similarity matrix</entity>. By applying some <entity>constraints</entity> on the chronological ordering of articles, an efficient <entity>threading algorithm</entity> that runs in <entity>0(n) time</entity> (where n is the number of articles) is obtained. The constructed <entity>graph</entity> is visualized with <entity>words</entity> that represent the <entity>topics</entity> of the <entity>threads</entity>, and <entity>words</entity> that represent new <entity>information</entity> in each article. The <entity>threading technique</entity> is suitable for Webcasting (push) applications. A <entity>threading server</entity> determines relationships among articles from various news sources, and creates files containing their threading information. This information is represented in <entity>eXtended Markup Language (XML)</entity>, and can be visualized on most <entity>Web browsers</entity>. The <entity>XML-based representation</entity> and a current <entity>prototype</entity> are described in this paper.
</abstract></paper>

<paper id="P98-1113"><title>
A Flexible <entity>Example-Based Parser</entity> Based on the <entity>SSTC</entity>
</title>
<abstract>
In this paper we sketch an approach for <entity>Natural Language parsing</entity>. Our approach is an <entity>example-based approach</entity>, which relies mainly on <entity>examples</entity> that already parsed to their <entity>representation structure</entity>, and on the <entity>knowledge</entity> that we can get from these <entity>examples</entity> the required information to parse a new <entity>input sentence</entity>. In our approach, <entity>examples</entity> are annotated with the <entity>Structured String Tree Correspondence (SSTC) annotation schema</entity> where each <entity>SSTC</entity> describes a <entity>sentence</entity>, a <entity>representation tree</entity> as well as the <entity>correspondence</entity> between <entity>substrings</entity> in the <entity>sentence</entity> and <entity>subtrees</entity> in the <entity>representation tree</entity>. In the process of parsing, we first try to build <entity>subtrees</entity> for <entity>phrases</entity> in the <entity>input sentence</entity> which have been successfully found in the <entity>example-base</entity> - a bottom up approach. These <entity>subtrees</entity> will then be combined together to form a <entity>single rooted representation tree</entity> based on an <entity>example</entity> with similar <entity>representation structure</entity> - a top down approach.Keywords:
</abstract></paper>

</corpus>