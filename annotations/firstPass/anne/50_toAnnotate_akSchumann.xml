<?xml version="1.0" encoding="UTF-8"?>
<corpus>
<corpus><paper id="P06-1012" for="P06-1010"><title>
Estimating <entity>Class</entity> Priors In <entity>Domain Adaptation</entity> For <entity>Word Sense Disambiguation</entity></title><abstract>
Instances of a <entity>word</entity> drawn from different <entity>domains</entity> may have <entity>different sense</entity> priors (the proportions of the different senses of a <entity>word</entity>). This in turn <entity>affects</entity> the <entity>accuracy</entity> of <entity>word sense disambiguation</entity> (WSD) <entity>systems</entity> <entity>trained</entity> and <entity>applied</entity> on different <entity>domains</entity>. This <entity>paper</entity> presents a <entity>method</entity> to estimate the <entity>sense</entity> priors of <entity>words</entity> drawn from a new <entity>domain</entity>, and highlights the <entity>importance</entity> of using well calibrated <entity>probabilities</entity> when <entity>performing</entity> these <entity>estimations</entity>. By using well calibrated <entity>probabilities</entity>, we are able to estimate the <entity>sense</entity> priors effectively to achieve significant <entity>improvements</entity> in WSD <entity>accuracy</entity>.
</abstract></paper>

<paper id="C86-1105" for="C86-1106"><title>
An Attempt To <entity>Automatic</entity> <entity>Thesaurus</entity> <entity>Construction</entity> From An Ordinary <entity>Japanese</entity> <entity>Language</entity> <entity>Dictionary</entity></title><abstract>
How to obtain hierarchical <entity>relations</entity>(e.g. superordinate -hyponym <entity>relation</entity>, <entity>synonym</entity> <entity>relation</entity>) is one of the most important <entity>Problems</entity> for <entity>thesaurus</entity> <entity>construction</entity>.   A <entity>pilot</entity> <entity>system</entity> for <entity>extracting</entity> these <entity>relations</entity> automatically from an ordinary <entity>Japanese</entity> <entity>language</entity> <entity>dictionary</entity> ( <PERSON>Shinmeikai Koku-gojiten</PERSON> , published by Sansei-do, in <entity>machine</entity> readable fori)
</abstract></paper>
<paper id="C86-1021"><title>
The <entity>Transfer</entity> <entity>Phase</entity> Of Mu <entity>Machine Translation System</entity></title><abstract>
The interlingual <entity>approach</entity> to MT has been repeatedly advocated by <entity>researchers</entity> originally interested in <entity>natural language understanding</entity> who take <entity>machine translation</entity> to be one possible <entity>application</entity>. However, not only the <entity>ambiguity</entity> but also the vagueness which every <entity>natural language</entity> inevitably has leads this <entity>approach</entity> into essenti.al <entity>difficulties</entity>. In <entity>contrast</entity>, our <entity>project</entity>, the <entity>Mu-project</entity>, adopts the <entity>transfer</entity> <entity>approach</entity> as the <entity>basic</entity> <entity>framework</entity> of MT. This <entity>paper</entity> describes the <entity>detailed</entity> <entity>construction</entity> of the <entity>transfer</entity> <entity>phase</entity> of our <entity>system</entity> from <entity>Japanese</entity> to <entity>English</entity>, and gives some <entity>examples</entity> of <entity>problems</entity> which seem difficult to treat  in the  interlingual <entity>approach</entity>. The <entity>basic</entity> <entity>design</entity> <entity>principles</entity> of the <entity>transfer</entity> <entity>phase</entity> of our <entity>system</entity> have already been <entity>mentioned</entity> in (1) (2). Some of the <entity>principles</entity> which are relevant to the <entity>topic</entity> of  this <entity>paper</entity> are: (a) Multiple <entity>Layer</entity> of Grammars (b) Multiple <entity>Layer</entity> <entity>Presentation</entity> (c) <entity>Lexicon</entity> Driven <entity>Processing</entity> (d) <entity>Form-</entity>Oriented <entity>Dictionary</entity> <entity>Description</entity> This <entity>paper</entity> also shows how these <entity>principles</entity> are realized in the <entity>current</entity> <entity>system</entity>. annotation <entity>parts</entity> of the dependent <entity>nodes</entity>, which are to be utilized in the subsequent recursion <entity>steps</entity>. For <entity>language pairs</entity> such as <entity>Japanese</entity> and <entity>English</entity> which belong to quite different <entity>language</entity> families, however, the <entity>lexical</entity> <entity>transfer</entity> is not so straightforward. It often happens that single <entity>lexical items</entity> of SL correspond to <entity>complex</entity> <entity>expressions</entity> of TL and <entity>vice</entity> versa. Furthermore, certain <entity>structural</entity> changes are also <entity>required</entity>. Because the M TP recursively <entity>transfers</entity> <entity>sub-structures</entity> governed by single <entity>nodes</entity>, certain global <entity>structural</entity> <entity>differences</entity> cannot be naturally treated. Such global changes are <entity>dealt</entity> with by the Pre-TP and Post-TP sub-phases   (See   <entity>section</entity> 7). W.   ..... g_
</abstract></paper>

<paper id="C04-1058"><title>
Why Nitpicking Works: <entity>Evidence</entity> For Occam's Razor In <entity>Error</entity> Correctors
</title><abstract>
"Empirical <entity>experience</entity> and <entity>observations</entity> have shown us when powerful and highly tunable <entity>classifiers</entity> such as <entity>maximum entropy</entity> <entity>classifiers</entity>, boosting and SVMs are <entity>applied</entity> to <entity>language processing</entity> <entity>tasks</entity>, it is possible to achieve high <entity>accuracies</entity>, but eventually their <entity>performances</entity> all tend to plateau out at around the same point. To further <entity>improve</entity> <entity>performance</entity>, various <entity>error</entity> <entity>correction</entity> <entity>mechanisms</entity> have been <entity>developed</entity>, but in <entity>practice</entity>, most of them cannot be relied on to predictably <entity>improve</entity> <entity>performance</entity> on unseen data; indeed, depending upon the <entity>test set</entity>, they are as likely to degrade <entity>accuracy</entity> as to <entity>improve</entity> it. This <entity>problem</entity> is especially severe if the <entity>base</entity> <entity>classifier</entity> has already been finely tuned. In recent work, we introduced N-fold Templated Piped <entity>Correction</entity>, or NTPC (""nitpick""), an intriguing <entity>error</entity> corrector that is <entity>designed</entity> to work in these extreme operating conditions. Despite its <entity>simplicity</entity>, it consistently and robustly <entity>improves</entity> the <entity>accuracy</entity> of existing highly accurate <entity>base</entity> <entity>models</entity>. This <entity>paper</entity> investigates some of the more surprising <entity>claims</entity> made by NTPC, and presents <entity>experiments</entity> <entity>supporting</entity> an Occam's Razor <entity>argument</entity> that more <entity>complex</entity> <entity>models</entity> are damaging or unnecessary in <entity>practice</entity>. "
</abstract></paper>

<paper id="N06-1007"><title><entity>Acquisition</entity> Of <entity>Verb</entity> <entity>Entailment</entity> From <entity>Text</entity></title><abstract>
The <entity>study</entity> addresses the <entity>problem</entity> of <entity>automatic</entity> <entity>acquisition</entity> of <entity>entailment</entity> <entity>relations</entity> between <entity>verbs</entity>. While this <entity>task</entity> has much in <entity>common</entity> with paraphrases <entity>acquisition</entity> which aims to discover <entity>semantic</entity> <entity>equivalence</entity> between <entity>verbs</entity>, the <entity>main</entity> <entity>challenge</entity> of <entity>entailment</entity> <entity>acquisition</entity> is to capture asymmetric, or directional, <entity>relations</entity>. Motivated by the <entity>intuition</entity> that it often underlies the local <entity>structure</entity> of coherent <entity>text</entity>, we <entity>develop</entity> a <entity>method</entity> that discovers <entity>verb</entity> en-tailment using <entity>evidence</entity> about <entity>discourse relations</entity> between <entity>clauses</entity> available in a parsed <entity>corpus</entity>. In <entity>comparison</entity> with earlier work, the proposed <entity>method</entity> covers a much wider range of <entity>verb</entity> <entity>entailment</entity> <entity>types</entity> and learns the <entity>mapping</entity> between <entity>verbs</entity> with highly varied <entity>argument structures</entity>.
</abstract></paper>

<paper id="A00-2023"><title><entity>Forest-</entity><entity>Based</entity> <entity>Statistical</entity> <entity>Sentence Generation</entity></title><abstract>
This <entity>paper</entity> presents a new <entity>approach</entity> to <entity>statistical</entity> <entity>sentence generation</entity> in which <entity>alternative</entity> <entity>phrases</entity> are represented as packed sets of <entity>trees</entity>, or <entity>forests</entity>, and then <entity>ranked</entity> statistically to choose the best one. This <entity>representation</entity> offers <entity>advantages</entity> in compactness and in the <entity>ability</entity> to represent <entity>syntactic information</entity>. It also facilitates more efficient <entity>statistical</entity> <entity>ranking</entity> than a previous <entity>approach</entity> to <entity>statistical</entity> <entity>generation</entity>. An efficient ranking <entity>algorithm</entity> is described, together with <entity>experimental</entity> <entity>results</entity> showing significant <entity>improvements</entity> over <entity>simple</entity> <entity>enumeration</entity> or a <entity>lattice-based approach</entity>.
</abstract></paper>

<paper id="X98-1022"><title>
An <entity>NTU-Approach</entity> To <entity>Automatic</entity> <entity>Sentence</entity> <entity>Extraction</entity> For <entity>Summary</entity> <entity>Generation</entity></title><abstract><entity>Automatic summarization</entity> and <entity>information extraction</entity> are two important Internet services. MUC and SUMMAC play their appropriate <entity>roles</entity> in the next <entity>generation</entity> Internet. This <entity>paper</entity> <entity>focuses</entity> on the <entity>automatic summarization</entity> and proposes two different <entity>models</entity> to <entity>extract</entity> <entity>sentences</entity> for <entity>summary</entity> <entity>generation</entity> under two <entity>tasks</entity> initiated by SUMMAC-1. For <entity>categorization</entity> <entity>task</entity>, positive <entity>feature vectors</entity> and negative <entity>feature vectors</entity> are used cooperatively to <entity>construct</entity> generic, indicative <entity>summaries</entity>. For adhoc <entity>task</entity>, a <entity>text</entity> <entity>model</entity> <entity>based</entity> on <entity>relationship</entity> between <entity>nouns</entity> and <entity>verbs</entity> is used to filter out irrelevant <entity>discourse</entity> <entity>segment</entity>, to <entity>rank</entity> relevant <entity>sentences</entity>, and to <entity>generate</entity> the <entity>user-directed</entity> <entity>summaries</entity>. The <entity>result</entity> shows that the NormF of the best <entity>summary</entity> and that of the fixed <entity>summary</entity> for adhoc <entity>tasks</entity> are 0.456 and 0. 447. The NormF of the best <entity>summary</entity> and that of the fixed <entity>summary</entity> for <entity>categorization</entity> <entity>task</entity> are 0.4090 and 0.4023. Our <entity>system</entity> outperforms the average <entity>system</entity> in <entity>categorization</entity> <entity>task</entity> but does a <entity>common</entity> job in adhoc <entity>task</entity>.
</abstract></paper>

<paper id="P98-2213" for="P98-2215"><title>
A <entity>Method</entity> for Relating Multiple <entity>Newspaper</entity> Articles by Using Graphs, and Its <entity>Application</entity> to Webcasting
</title><abstract>
This <entity>paper</entity> describes <entity>methods</entity> for relating (<entity>threading</entity>) multiple <entity>newspaper</entity> articles, and for visualizing various <entity>characteristics</entity> of them by using a directed graph. A set of articles is represented by a set of <entity>word</entity> <entity>vectors</entity>, and the <entity>similarity</entity> between the <entity>vectors</entity> is then calculated. The graph is <entity>constructed</entity> from the <entity>similarity</entity> <entity>matrix</entity>. By <entity>applying</entity> some <entity>constraints</entity> on the chronological <entity>ordering</entity> of articles, an efficient <entity>threading</entity> <entity>algorithm</entity> that runs in 0(n) <entity>time</entity> (where n is the <entity>number</entity> of articles) is obtained. The <entity>constructed</entity> graph is visualized with <entity>words</entity> that represent the <entity>topics</entity> of the <entity>threads</entity>, and <entity>words</entity> that represent new <entity>information</entity> in each article. The <entity>threading</entity> <entity>technique</entity> is suitable for Webcasting (push) <entity>applications</entity>. A <entity>threading</entity> <entity>server</entity> determines <entity>relationships</entity> among articles from various <entity>news</entity> <entity>sources</entity>, and creates files containing their <entity>threading</entity> <entity>information</entity>. This <entity>information</entity> is represented in eXtended <entity>Markup</entity> <entity>Language</entity> (XML), and can be visualized on most Web <entity>browsers</entity>. The XML-based <entity>representation</entity> and a <entity>current</entity> <entity>prototype</entity> are described in this <entity>paper</entity>.
</abstract></paper>

<paper id="P98-1113"><title>
A Flexible <entity>Example-</entity><entity>Based</entity> <entity>Parser</entity> <entity>Based</entity> on the SST C
</title><abstract>
In this <entity>paper</entity> we sketch an <entity>approach</entity> for <entity>Natural Language</entity> <entity>parsing</entity>. Our <entity>approach</entity> is an <entity>example-based approach</entity>, which relies mainly on <entity>examples</entity> that already parsed to their <entity>representation</entity> <entity>structure</entity>, and on the <entity>knowledge</entity> that we can get from these <entity>examples</entity> the required <entity>information</entity> to <entity>parse</entity> a new <entity>input</entity> <entity>sentence</entity>. In our <entity>approach</entity>, <entity>examples</entity> are annotated with the Structured <entity>String</entity> <entity>Tree</entity> <entity>Correspondence</entity> (SSTC) annotation <entity>schema</entity> where each SSTC describes a <entity>sentence</entity>, a <entity>representation</entity> <entity>tree</entity> as well as the <entity>correspondence</entity> between substrings in the <entity>sentence</entity> and subtrees in the <entity>representation</entity> <entity>tree</entity>. In the <entity>process</entity> of <entity>parsing</entity>, we first try to build subtrees for <entity>phrases</entity> in the <entity>input</entity> <entity>sentence</entity> which have been successfully found in the <entity>example-base</entity> - a bottom up <entity>approach</entity>. These subtrees will then be combined together to <entity>form</entity> a single rooted <entity>representation</entity> <entity>tree</entity> <entity>based</entity> on an <entity>example</entity> with similar <entity>representation</entity> <entity>structure</entity> - a top down <entity>approach</entity>.Keywords:
</abstract></paper>

</corpus>