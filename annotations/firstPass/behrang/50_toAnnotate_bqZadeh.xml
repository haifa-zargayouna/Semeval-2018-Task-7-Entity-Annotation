<?xml version="1.0" encoding="UTF-8"?>
<corpus>

<paper id="P02-1060"><title><entity>Named</entity> <entity>Entity</entity> <entity>Recognition</entity> Using An HMM-Based <entity>Chunk</entity> Tagger
</title><abstract>
This <entity>paper</entity> proposes a <entity>Hidden Markov Model</entity> (HMM) and an HMM-based <entity>chunk</entity> tagger, from which a <entity>named</entity> <entity>entity</entity> (NE) <entity>recognition</entity> (NER) <entity>system</entity> is built to recognize and classify <entity>names</entity>, <entity>times</entity> and numerical <entity>quantities</entity>. Through the HMM, our <entity>system</entity> is able to <entity>apply</entity> and integrate four <entity>types</entity> of internal and external <entity>evidences</entity>: 1) <entity>simple</entity> deterministic internal <entity>feature</entity> of the <entity>words</entity>, such as <entity>capitalization</entity> and digitalization; 2) internal <entity>semantic feature</entity> of important triggers; 3) internal <entity>gazetteer</entity> <entity>feature</entity>; 4) external macro <entity>context feature</entity>. In this way, the NER <entity>problem</entity> can be resolved effectively. <entity>Evaluation</entity> of our <entity>system</entity> on MUC-6 and MUC-7 <entity>English</entity> NE <entity>tasks</entity> achieves F-measures of 96.6% and 94.1% respectively. It shows that the <entity>performance</entity> is significantly better than <entity>reported</entity> by any other <entity>machine-learning</entity> <entity>system</entity>. Moreover, the <entity>performance</entity> is even consistently better than those <entity>based</entity> on handcrafted <entity>rules</entity>.
</abstract></paper>

<paper id="P02-1008"><title>
Phonological <entity>Comprehension</entity> And The <entity>Compilation</entity> Of Optimality <entity>Theory</entity></title><abstract>
This <entity>paper</entity> ties up some loose ends in finite-state Optimality <entity>Theory</entity>. First, it discusses how to <entity>perform</entity> <entity>comprehension</entity> under Optimality <entity>Theory</entity> grammars consisting of finite-state <entity>constraints</entity>. <entity>Comprehension</entity> has not been much <entity>studied</entity> in OT; we show that unlike production, it does not always <entity>yield</entity> a regular set, making finite-state <entity>methods</entity> inapplicable. However, after giving a suitably flexible <entity>presentation</entity> of OT, we show carefully how to treat <entity>comprehension</entity> under recent <entity>variants</entity> of OT in which grammars can be compiled into finite-state transducers. We then unify these <entity>variants</entity>, showing that <entity>compilation</entity> is possible ifall <entity>components</entity> ofthe grammar are regular <entity>relations</entity>, <entity>including</entity> the harmony <entity>ordering</entity> on scored <entity>candidates</entity>.
</abstract></paper>

<paper id="C96-2213"><title>
Using A Hybrid <entity>System</entity> Of <entity>Corpus-</entity> And <entity>Knowledge-</entity><entity>Based</entity> <entity>Techniques</entity> To Automate The <entity>Induction</entity> Of A <entity>Lexical</entity> Sublanguage Grammar
</title><abstract>
Porting a <entity>Natural Language Processing</entity> (NLP) <entity>system</entity> to a new <entity>domain</entity> remains one of the bottlenecks in <entity>syntactic</entity> <entity>parsing</entity>, because of the <entity>amount</entity> of <entity>effort</entity> <entity>required</entity> to fix <entity>gaps</entity> in the <entity>lexicon</entity>, and to attune the existing grammar to the idiosyncra-cios of the new sublanguage. This <entity>paper</entity> shows how the <entity>process</entity> of fitting a lexicalized grammar to a <entity>domain</entity> can be automated to a great <entity>extent</entity> by using a hybrid <entity>system</entity> that combines traditional <entity>knowledge-based</entity> <entity>techniques</entity> with a <entity>corpus-based approach</entity>.
</abstract></paper>

<paper id="C04-1102"><title>
Detecting Transliterated Orthographic Variants Via Two <entity>Similarity</entity> Metrics
</title><abstract>
We propose a <entity>detection</entity> <entity>method</entity> for orthographic <entity>variants</entity> caused by <entity>transliteration</entity> in a large <entity>corpus</entity>. The <entity>method</entity> employs two <entity>similarities</entity>. One is <entity>string</entity> <entity>similarity</entity> <entity>based</entity> on edit <entity>distance</entity>. The other is contextual <entity>similarity</entity> by a <entity>vector space model</entity>. <entity>Experimental</entity> <entity>results</entity> show that the <entity>method</entity> <entity>performed</entity> a 0.889 F-measure in an open <entity>test</entity>.
</abstract></paper>

<paper id="N06-1018"><title><entity>Understanding</entity> Temporal Expressions In Emails
</title><abstract>
Recent years have seen <entity>increasing</entity> <entity>research</entity> on <entity>extracting</entity> and using temporal <entity>information</entity> in <entity>natural language</entity> <entity>applications</entity>. However most of the works found in the <entity>literature</entity> have <entity>focused</entity> on identifying and understanding temporal <entity>expressions</entity> in newswire <entity>texts</entity>. anchoring emails. <entity>Time</entity> <entity>Calculus</entity> for <entity>Natural Language</entity></abstract></paper>

<paper id="H89-2066"><title><entity>Research</entity> And <entity>Development</entity> For <entity>Spoken Language</entity> <entity>Systems</entity></title><abstract>
The <entity>goal</entity> of this <entity>research</entity> is to <entity>develop</entity> a spoken <entity>language system</entity> that will demonstrate the <entity>usefulness</entity> of voice <entity>input</entity> for interactive <entity>problem</entity> <entity>solving</entity>. The <entity>system</entity> will accept continuous <entity>speech</entity>, and will handle multiple speakers without explicit speaker enrollment. <entity>Combining</entity> <entity>speech recognition</entity> and <entity>natural language processing</entity> to achieve <entity>speech</entity> <entity>understanding</entity>, the <entity>system</entity> will be demonstrated in an <entity>application</entity> <entity>domain</entity> relevant to the DoD. The <entity>objective</entity> of this <entity>project</entity> is to <entity>develop</entity> a <entity>robust</entity> and <entity>high-performance</entity> <entity>speech recognition system</entity> using a <entity>segment-based approach</entity> to phonetic <entity>recognition</entity>. The <entity>recognition system</entity> will eventually be integrated with <entity>natural language processing</entity> to achieve spoken <entity>language understanding</entity>.
</abstract></paper>

<paper id="J81-1002"><title><entity>Computer</entity> <entity>Generation</entity> Of Multiparagraph <entity>English</entity> <entity>Text</entity></title><abstract>
This <entity>paper</entity> <entity>reports</entity> recent <entity>research</entity> into <entity>methods</entity> for creating <entity>natural language text</entity>. A new <entity>processing</entity> <entity>paradigm</entity> <entity>called</entity> <entity>Fragment-and-</entity>Compose has been created and an <entity>experimental</entity> <entity>system</entity> <entity>implemented</entity> in it. The <entity>knowledge</entity> to be expressed in <entity>text</entity> is first divided into small propositional <entity>units</entity>, which are then composed into appropriate <entity>combinations</entity> and converted into <entity>text</entity>.KDS (<entity>Knowledge</entity> Delivery <entity>System</entity>), which embodies this <entity>paradigm</entity>, has distinct <entity>parts</entity> devoted to <entity>creation</entity> of the propositional <entity>units</entity>, to <entity>organization</entity> of the <entity>text</entity>, to prevention of excess <entity>redundancy</entity>, to <entity>creation</entity> of <entity>combinations</entity> of <entity>units</entity>, to <entity>evaluation</entity> of these <entity>combinations</entity> as potential <entity>sentences</entity>, to <entity>selection</entity> of the best among competing <entity>combinations</entity>, and to <entity>creation</entity> of the final <entity>text</entity>. The <entity>Fragment-and-</entity>Compose <entity>paradigm</entity> and the <entity>computational</entity> <entity>methods</entity> of KDS are described.
</abstract></paper>

<paper id="C90-1002" for="C90-2037"><title><entity>Design</entity> Of A Hybrid Deterministic <entity>Parser</entity></title><abstract>
A deterministic <entity>parser</entity> is under <entity>development</entity> which represents a departure from traditional deterministic <entity>parsers</entity> in that it combines both symbolic and connectionist <entity>components</entity>. The conncctionist <entity>component</entity> is <entity>trained</entity> either from <entity>patterns</entity> derived from the <entity>rules</entity> of a deterministic grammar. The <entity>development</entity> and <entity>evolution</entity> of such a hybrid <entity>architecture</entity> has lead to a <entity>parser</entity> which is superior to any known deterministic <entity>parser</entity>. <entity>Experiments</entity> are described and powerful <entity>training</entity> <entity>techniques</entity> are demonstrated that permit <entity>decision-making</entity> by the connectionist <entity>component</entity> in the <entity>parsing</entity> <entity>process</entity>. This <entity>approach</entity> has permitted some <entity>simplifications</entity> to the <entity>rules</entity> of other deterministic <entity>parsers</entity>, <entity>including</entity> the <entity>elimination</entity> of <entity>rule</entity> packets and priorities. Furthermore, <entity>parsing</entity> is <entity>performed</entity> more robustly and with more tolerance for <entity>error</entity>. <entity>Data</entity> are presented which show how a connectionist (neural) <entity>network</entity> <entity>trained</entity> with <entity>linguistic rules</entity> can <entity>parse</entity> both expected (grammatical) <entity>sentences</entity> as well as some novel (ungrammatical or lexically ambiguous) <entity>sentences</entity>.
</abstract></paper>

</corpus>